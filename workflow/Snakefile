"""
@author: Katherine Eaton
SARS-CoV-2 Recombinant Detection Pipeline.
"""

# ------------------------------------------------------------------------------
#                             Modules and Packages                             #
# ------------------------------------------------------------------------------
import os
import copy
import textwrap
import datetime
import subprocess
import pandas as pd
import json

# Enforce minimum version
from snakemake.utils import min_version
min_version("7.3.6")

# ------------------------------------------------------------------------------
# Setup
# ------------------------------------------------------------------------------

today = datetime.date.today()

# Begin Code: https://github.com/nextstrain/ncov/blob/master/Snakefile

# Store the user's configuration prior to loading defaults. We need to make a deep
# copy because Snakemake will deep merge the dictionary later, modifying the values
# of a reference or shallow copy. Note that this loading of
# the user's config prior to the defaults below depends on the order Snakemake
# loads its configfiles. Specifically, the order of config loading is:
#
# 1. First, configfile arguments are loaded and config is built from these [1].
# 2. Then, config arguments are loaded and override existing config keys [2].
# 3. Then, the Snakefile is parsed and configfile directive inside the Snakefile is processed [3].
#    When configfile is loaded from the directive in the Snakefile, the config
#    dictionary is deep merged with the files [4] from the externally provided
#    config files. This is the only place the deep merge happens using the
#    update_config function [5].

# Load the default parameters
configfile: "defaults/parameters.yaml"

if isinstance(config.get("builds"), list):
    config["builds"] = OrderedDict((v["name"], v) for v in config["builds"])
if isinstance(config.get("rule_params"), list):
    config["rule_params"] = OrderedDict((v["name"], v) for v in config["rule_params"])

# Check for missing builds
if "builds" not in config:
    logger.error("ERROR: Your workflow does not define any builds to start with.")
    logger.error("Update your configuration file (e.g., 'builds.yaml') to define at least one build as follows and try running the workflow again:")
    logger.error(textwrap.indent(
        f"\nbuilds:\n  name: example-build\n",
        "  "
    ))
    sys.exit(1)

# End Code: https://github.com/nextstrain/ncov/blob/master/Snakefile

default_keys = ["builds", "rule_params"]
default_build_keys = ["name"]

# Set default rule parameters for each build
for build in config["builds"]:
  for rule in config["rule_params"]:
    # Case 1. There are no params for this rule in the build
    if rule not in config["builds"][build]:
      config["builds"][build][rule] = config["rule_params"][rule]
    # Case 2. There are some params for this rule in the build
    else:
      for param in config["rule_params"][rule]:
        # Case 3. This param has not been specified
        if param not in config["builds"][build][rule]:
          config["builds"][build][rule][param] = config["rule_params"][rule][param]

# Store list of builds for rule wildcard constraints
BUILDS = list(config["builds"].keys())

# ------------------------------------------------------------------------------
#  Functions
# ------------------------------------------------------------------------------

def _inputs(build):

  inputs = {}

  # Metadata
  metadata = "data/{build}/metadata.tsv"
  if "metadata" in config["builds"][build].keys():
    metadata = config["builds"][build]["metadata"]
    if metadata:
      inputs["metadata"] = metadata

  # Sequences
  sequences = "data/{build}/sequences.fasta"
  if "sequences" in config["builds"][build].keys():
    sequences = config["builds"][build]["sequences"]
    if sequences:
      inputs["sequences"] = sequences

  return inputs

# ------------------------------------------------------------------------------
#  Default Target
# ------------------------------------------------------------------------------

rule all:
  """
  Default workflow targets.
  """
  input:
    # Config files
    expand("results/{build_name}/config.json",
      build_name=BUILDS,
      ),
    # Stage 1: Nextclade
    expand("results/{build_name}/nextclade/alignment.fasta",
      build_name=BUILDS,
      ),
    # Stage 2: sc2rf
    expand("results/{build_name}/sc2rf/stats.tsv",
      build_name=BUILDS,
      ),
    # Stage 3: Reports
    expand("results/{build_name}/report/report.xlsx",
      build_name=BUILDS,
      ),
    expand("results/{build_name}/report_historical/report.pptx",
              build_name=BUILDS,
              ),
    # Stage 4: Validation
    expand("results/{build_name}/validate/validation.tsv",
      build_name=BUILDS,
      ),

# ------------------------------------------------------------------------------
#  Accessory Rules
# ------------------------------------------------------------------------------

rule print_config:
  """
  Print full config.
  """
  message: "Printing full config."
  threads: 1
  resources:
    cpus = 1,
  run:
    # Print the config
    config_json = json.dumps(config, indent = 2)
    print(config_json)


# ------------------------------------------------------------------------------
rule save_config:
  """
  Save full config.
  """
  message: "Saving full config."
  output:
    json = "results/{build}/config.json",
  threads: 1
  resources:
    cpus = 1,
  params:
    output_dir = "results/{build}/",
  run:
    config_copy = copy.deepcopy(config)
    # Remove other builds from config
    for build in config["builds"]:
      if build != wildcards.build:
        del config_copy["builds"][build]

    config_json = json.dumps(config_copy, indent = 2)

    # Create output directory
    if not os.path.exists(params.output_dir):
      os.mkdir(params.output_dir)

    # Save to json file
    outfile_path = output.json
    with open(outfile_path, "w") as outfile:
      outfile.write(config_json)

# ------------------------------------------------------------------------------
rule help:
  """
  Print list of all rules and targets with help.
  """
  threads: 1
  resources:
    cpus = 1,
  run:
    for rule in workflow.rules:
      print("-" * 160)
      print("rule: ", rule.name )
      if rule.docstring:
          print(rule.docstring)
      if rule._input:
          print("\tinput:")
          for in_file in rule.input:
              print("\t\t" + str(in_file))
          for in_file in rule.input.keys():
              print("\t\t" + in_file + ": " + str(rule.input[in_file]))
      if rule._output:
          print("\toutput:")
          for out_file in rule.output:
              print("\t\t" + out_file)
          for out_file in rule.output.keys():
              print("\t\t" + out_file + ": " + str(rule.output[out_file]))
      if rule._params:
          print("\tparams:")
          for param in rule.params.keys():
              print("\t\t" + param + ": " + str(rule.params[param]))
      if rule.resources:
          print("\tresources:")
          for resource in rule.resources.keys():
              print("\t\t" + resource + ": " + str(rule.resources[resource]))
      if rule.conda_env:
          print("\t\tconda: ", rule.conda_env)
      if rule._log:
          print("\t\tlog: ", rule._log)

# -----------------------------------------------------------------------------
#  STAGE 0: Setup
# -----------------------------------------------------------------------------

rule_name = "issues_download"
rule issues_download:
  """Download pango-designation issues."""

  message: """Downloading pango-designation issues.\n
  log:     {log}
  issues:  {output.issues}
  plot:    {output.breakpoints_png}
  """

  input:
    breakpoints      = "resources/breakpoints.tsv",
  output:
    issues           = "resources/issues.tsv",
    issue_to_lineage = "resources/issue_to_lineage.tsv",
    breakpoints_png  = "resources/breakpoints_clade.png",
    breakpoints_svg  = "resources/breakpoints_clade.svg",
    breakpoints_tsv  = "resources/breakpoints_clade.tsv",
  params:
    outdir = "resources",
  threads: 1
  resources:
    cpus = 1,
  benchmark:
    "benchmarks/{rule}/{today}.tsv".format(today=today, rule=rule_name),
  log:
    "logs/{rule}/{today}.log".format(today=today, rule=rule_name),
  shell:
    """
    python3 scripts/download_issues.py --breakpoints {input.breakpoints} 1> {output.issues} 2> {log};
    csvtk cut -t -f "issue,lineage" {output.issues} | tail -n+2   1> {output.issue_to_lineage} 2>> {log};
    python3 scripts/plot_breakpoints.py --lineages {input.breakpoints} --outdir {params.outdir} --autoscale >> {log} 2>&1;
    """

# -----------------------------------------------------------------------------
rule_name = "lineage_tree"
rule lineage_tree:
  """Construct a nomenclature tree of lineages."""

  message: """Constructing a nomenclature tree of lineages.\n
  log:     {log}
  tree:    {output.tree}
  """

  wildcard_constraints:
    # The tag will always begin with the year (ex. 2022)
    tag         = "([0-9]){4}.*",
  output:
    tree = "resources/tree.nwk",
  threads: 1
  resources:
    cpus = 1,
  benchmark:
    "benchmarks/{rule}/{today}.tsv".format(today=today, rule=rule_name),
  log:
    "logs/{rule}/{today}.log".format(today=today, rule=rule_name),
  shell:
    """
    python3 scripts/lineage_tree.py --output {output.tree} > {log};
    """

# -----------------------------------------------------------------------------
rule_name = "nextclade_dataset"
rule nextclade_dataset:
  """Download Nextclade dataset."""

  message: """Downloading Nextclade dataset.\n
  log:     {log}
  dataset: {output.dataset_dir}
  """

  wildcard_constraints:
    # The tag will always begin with the year (ex. 2022)
    tag         = "([0-9]){4}.*",
  output:
    dataset_dir = directory("data/{dataset}_{tag}"),
  threads: 1
  resources:
    cpus = 1,
  benchmark:
    "benchmarks/{rule}/{{dataset}}_{{tag}}_{today}.tsv".format(today=today, rule=rule_name),
  log:
    "logs/{rule}/{{dataset}}_{{tag}}_{today}.log".format(today=today, rule=rule_name),
  shell:
    """
    # Download dataset
    nextclade dataset get --name {wildcards.dataset} --tag {wildcards.tag} --output-dir {output.dataset_dir} > {log} 2>&1;
    """

# ------------------------------------------------------------------------------
#  STAGE 1: Nextclade QC
# ------------------------------------------------------------------------------

rule_name = "nextclade"

def _inputs_nextclade(build, prefix):
  inputs = {}

  if prefix == "nextclade_no-recomb":
    dataset = config["builds"][build]["nextclade_dataset"]["dataset_no-recomb"]
    tag = config["builds"][build]["nextclade_dataset"]["tag_no-recomb"]
  elif prefix == "nextclade_immune-escape":
    dataset = config["builds"][build]["nextclade_dataset"]["dataset_immune-escape"]
    tag = config["builds"][build]["nextclade_dataset"]["tag_immune-escape"]
  else:
    dataset = config["builds"][build]["nextclade_dataset"]["dataset"]
    tag = config["builds"][build]["nextclade_dataset"]["tag"]

  inputs["tag"] = tag
  inputs["dataset"] = dataset

  return inputs

rule nextclade:
  """Align sequences and perform QC with Nextclade."""

  message: """Aligning sequences and performing QC with Nextclade.\n
  build:     {wildcards.build}
  log:       {log}
  qc:        {output.qc}
  metadata:  {output.metadata}
  alignment: {output.alignment}
  """

  wildcard_constraints:
    nextclade_prefix = "nextclade|nextclade_no-recomb|nextclade_immune-escape"
  input:
    dataset       = lambda wildcards:  "data/{dataset}_{tag}".format(
                      dataset=_inputs_nextclade(wildcards.build, wildcards.nextclade_prefix)["dataset"],
                      tag=_inputs_nextclade(wildcards.build, wildcards.nextclade_prefix)["tag"],
                      ),
    sequences     = lambda wildcards: _inputs(wildcards.build)["sequences"],
    metadata      = lambda wildcards: _inputs(wildcards.build)["metadata"],
  output:
    alignment     = "results/{build}/{nextclade_prefix}/alignment.fasta",
    qc            = "results/{build}/{nextclade_prefix}/qc.tsv",
    metadata      = "results/{build}/{nextclade_prefix}/metadata.tsv",
  params:
    outdir        = "results/{build}/{nextclade_prefix}",
    basename      = "nextclade",
    selection     = "fasta,tsv"
  benchmark:
    "benchmarks/{{nextclade_prefix}}/{{build}}_{today}.tsv".format(today=today),
  log:
    "logs/{{nextclade_prefix}}/{{build}}_{today}.log".format(today=today),
  shell:
    """
    # Align sequences
    nextclade run \
      --jobs {resources.cpus} \
      --input-dataset {input.dataset} \
      --output-all {params.outdir} \
      --output-selection {params.selection} \
      --output-tsv {output.qc} \
      --output-fasta {output.alignment} \
      --output-basename {params.basename} \
      {input.sequences} \
      >> {log} 2>&1;

    # Merge QC output with metadata
    csvtk rename -t -f "seqName" -n "strain" {output.qc} 2>> {log} \
      | csvtk merge -t -f "strain" {input.metadata} - \
      1> {output.metadata} 2>> {log};
    """

# -----------------------------------------------------------------------------
rule_name = "nextclade_recombinants"

# Parameters function
def _params_nextclade_recombinants(build):
  """Parse conditional parameters for rule nextclade_recombinants."""

  params = {}
  exclude_clades = config["builds"][build]["nextclade_recombinants"]["exclude_clades"]
  if exclude_clades: params["exclude_clades"] = ".*(" + "|".join(exclude_clades) + ").*"
  else: params["exclude_clades"] = "None"

  return params

rule nextclade_recombinants:
  """Extract recombinants from nextclade output."""

  message: """Extracting recombinant candidates from nextclade output.\n
  build:     {wildcards.build}
  log:       {log}
  alignment: {output.alignment}
  strains:   {output.strains}
  """

  input:
    alignment      = "results/{build}/nextclade/alignment.fasta",
    qc             = "results/{build}/nextclade/qc.tsv",
  output:
    alignment      = "results/{build}/nextclade/recombinants.fasta",
    strains        = "results/{build}/nextclade/recombinants.txt",
    qc             = "results/{build}/nextclade/recombinants.qc.tsv",
    non            = "results/{build}/nextclade/non-recombinants.qc.tsv",
  params:
    exclude_clades = lambda wildcards: _params_nextclade_recombinants(wildcards.build)["exclude_clades"],
    fields         = "clade,Nextclade_pango,qc.overallStatus,privateNucMutations.labeledSubstitutions",
    values         = ".*(recombinant|X|bad|mediocre|\|).*", # \| is a char for the labelSubstitutions column
  threads: 1
  resources:
    cpus = 1,
  benchmark:
    "benchmarks/{rule}/{{build}}_{today}.tsv".format(today=today, rule=rule_name),
  log:
    "logs/{rule}/{{build}}_{today}.log".format(today=today, rule=rule_name),
  shell:
    """
    # Extract recombinant strains
    csvtk grep -t -v -f "clade" -r -p "{params.exclude_clades}" {input.qc}  2> {log} \
      | csvtk grep -t -f "qc.mixedSites.status" -p "bad" -v 2>> {log} \
      | csvtk grep -t -f "{params.fields}" -r -p "{params.values}" 2>> {log} \
      | csvtk cut -t -f "seqName" 2>> {log} \
      | tail -n+2 \
      > {output.strains};
    # Extract recombinant sequences
    seqkit grep -f {output.strains} {input.alignment} 1> {output.alignment} 2>> {log};
    # Extract recombinant qc
    csvtk grep -t -P {output.strains} {input.qc} 1> {output.qc} 2>> {log};
    # Extract non-recombinants qc
    csvtk grep -t -P {output.strains} -v {input.qc} 1> {output.non} 2>> {log};
    """

# ------------------------------------------------------------------------------
rule_name = "rbd_levels"
rule rbd_levels:
  """Calculate the number of key RBD mutations."""

  message: """Calculating the number of key RBD mutations.\n
  build:     {wildcards.build}
  log:       {log}
  table:     {output.table}
  """

  input:
    nextclade      = "results/{build}/nextclade_immune-escape/qc.tsv",
    rbd_definition = "resources/rbd_levels.yaml",
  output:
    table      = "results/{build}/rbd_levels/rbd_levels.tsv",
  threads: 1
  resources:
    cpus = 1,
  benchmark:
    "benchmarks/{rule}/{{build}}_{today}.tsv".format(today=today, rule=rule_name),
  log:
    "logs/{rule}/{{build}}_{today}.log".format(today=today, rule=rule_name),
  shell:
    """
    python3 scripts/rbd_levels.py --rbd-definition {input.rbd_definition} --nextclade {input.nextclade} --output {output.table} --log {log};
    """


# ------------------------------------------------------------------------------
#  STAGE 3: sc2rf
# ------------------------------------------------------------------------------

rule_name = "sc2rf"

# Input function
def _inputs_sc2rf(build):
  """Parse conditional inputs for rule sc2rf."""

  inputs = {}
  exclude_negatives = config["builds"][build]["sc2rf"]["exclude_negatives"]

  # nextclade/alignment.fasta: positives, negatives, and false_positives
  # nextclade/recombinants.fasta: positives (and false_positives)

  # Option 1: Everything (positives, negatives, false_positives)
  if not exclude_negatives:
    inputs["alignment"] = "results/{build}/nextclade/alignment.fasta".format(build=build)

  # Option 2: Positives only
  else:
    inputs["alignment"] = "results/{build}/nextclade/recombinants.fasta".format(build=build)

  return inputs

def _params_sc2rf(build):
  """Parse conditional params for rule sc2rf."""

  params = {}

  # Add the arguments for each mode
  params["sc2rf_args"] = {}
  for mode_args in config["builds"][build]["sc2rf"]["mode"]:

      mode = list(mode_args.keys())[0]
      args = list(mode_args.values())[0]

      params["sc2rf_args"][mode] = args

  return params

# Snakemake rule
rule sc2rf:
  """
  Identify recombinants with sc2rf.
  """

  message: """Identifying recombinants with sc2rf.\n
  mode:        {wildcards.mode}
  build:       {wildcards.build}
  log:         {log}
  ansi:        {output.ansi}
  stats:       {output.csv}
  """

  input:
    alignment            = lambda wildcards: _inputs_sc2rf(wildcards.build)["alignment"],
  output:
    ansi                 = "results/{build}/sc2rf/ansi.{mode}.txt",
    csv                  = "results/{build}/sc2rf/stats.{mode}.csv",
  params:
    outdir               = "results/{build}",
    sc2rf_args           = lambda wildcards: _params_sc2rf(wildcards.build)["sc2rf_args"][wildcards.mode],
    max_name_length      = lambda wildcards: config["builds"][wildcards.build]["sc2rf"]["max_name_length"]
  threads: 1
  resources:
    cpus = 1,
  benchmark:
    "benchmarks/{rule}/{{build}}_{{mode}}_{today}.tsv".format(today=today, rule=rule_name),
  log:
    "logs/{rule}/{{build}}_{{mode}}_{today}.log".format(today=today, rule=rule_name),
  shell:
    """
    scripts/sc2rf.sh \
      --alignment {input.alignment} \
      --output-ansi {output.ansi} \
      --output-csv {output.csv} \
      --log {log} \
      --max-name-length {params.max_name_length} \
      {params.sc2rf_args};
    """

# -----------------------------------------------------------------------------
rule_name = "sc2rf_recombinants"

# Inputs function
def _inputs_sc2rf_recombinants(build):
  """Parse conditional inputs for rule sc2rf_recombinants."""

  inputs = {}

  inputs["ansi"] = []
  inputs["csv"] = []

  for mode_args in config["builds"][build]["sc2rf"]["mode"]:

      mode = list(mode_args.keys())[0]

      # Ansi input
      ansi_path = "results/{build}/sc2rf/ansi.{mode}.txt".format(
        build = build,
        mode = mode,
      )
      csv_path = "results/{build}/sc2rf/stats.{mode}.csv".format(
        build = build,
        mode = mode,
      )

      inputs["ansi"].append(ansi_path)
      inputs["csv"].append(csv_path)

  return inputs

# Parameters function
def _params_sc2rf_recombinants(build):
  """Parse conditional parameters for rule sc2rf_recombinants."""

  params = {}
  motifs = config["builds"][build]["sc2rf_recombinants"]["motifs"]
  if motifs: params["motifs"] = "--motifs {}".format(motifs)
  else: params["motifs"] = ""

  min_len = config["builds"][build]["sc2rf_recombinants"]["min_len"]
  if min_len: params["min_len"] = "--min-len {}".format(min_len)
  else: params["min_len"] = ""

  min_consec_allele = config["builds"][build]["sc2rf_recombinants"]["min_consec_allele"]
  if min_consec_allele: params["min_consec_allele"] = "--min-consec-allele {}".format(min_consec_allele)
  else: params["min_consec_allele"] = ""

  max_breakpoint_len = config["builds"][build]["sc2rf_recombinants"]["max_breakpoint_len"]
  if max_breakpoint_len: params["max_breakpoint_len"] = "--max-breakpoint-len {}".format(max_breakpoint_len)
  else: params["max_breakpoint_len"] = ""

  max_breakpoints = config["builds"][build]["sc2rf_recombinants"]["max_breakpoints"]
  if max_breakpoints: params["max_breakpoints"] = "--max-breakpoints {}".format(max_breakpoints)
  else: params["max_breakpoints"] = ""

  max_parents = config["builds"][build]["sc2rf_recombinants"]["max_parents"]
  if max_parents: params["max_parents"] = "--max-parents {}".format(max_parents)
  else: params["max_parents"] = ""

  dup_method = config["builds"][build]["sc2rf_recombinants"]["dup_method"]
  if dup_method: params["dup_method"] = "--dup-method {}".format(dup_method)
  else: params["dup_method"] = ""

  auto_pass = config["builds"][build]["sc2rf_recombinants"]["auto_pass"]
  if auto_pass: params["auto_pass"] = "--nextclade-auto-pass {}".format(",".join(auto_pass))
  else: params["auto_pass"] = ""

  lapis = config["builds"][build]["sc2rf_recombinants"]["lapis"]
  if lapis: params["lapis"] = "--lapis"
  else: params["lapis"] = ""

  # The metadata param will not be used if we are excluding negatives
  metadata = _inputs(build)["metadata"]
  exclude_negatives = config["builds"][build]["sc2rf"]["exclude_negatives"]
  if not exclude_negatives: params["metadata"] = "--metadata {}".format(metadata)
  else: params["metadata"] = ""

  return params

# Snakemake rule
rule sc2rf_recombinants:
  """Postprocessing the sc2rf output to identify recombinants."""

  message: """Postprocessing the sc2rf output to identify recombinants.\n
  build:       {wildcards.build}
  log:         {log}
  stats:       {output.stats}
  alignment:   {output.alignment}
  ansi:        {output.ansi}
  """

  input:
    ansi            = lambda wildcards: _inputs_sc2rf_recombinants(wildcards.build)["ansi"],
    csv             = lambda wildcards: _inputs_sc2rf_recombinants(wildcards.build)["csv"],
    alignment       = lambda wildcards: _inputs_sc2rf(wildcards.build)["alignment"],
    issues          = rules.issues_download.output.issues,
    nextclade       = "results/{build}/nextclade/qc.tsv",
    nextclade_no_recomb = "results/{build}/nextclade_no-recomb/qc.tsv",
    lineage_tree    = rules.lineage_tree.output.tree,
    metadata        = lambda wildcards: _inputs(wildcards.build)["metadata"],
  output:
    stats           = "results/{build}/sc2rf/stats.tsv",
    strains         = "results/{build}/sc2rf/recombinants.txt",
    alignment       = "results/{build}/sc2rf/recombinants.fasta",
    ansi            = "results/{build}/sc2rf/recombinants.ansi.txt",
    exclude         = "results/{build}/sc2rf/recombinants.exclude.tsv",
  params:
    outdir          = "results/{build}/sc2rf",
    prefix          = "recombinants",
    min_len         = lambda wildcards: _params_sc2rf_recombinants(wildcards.build)["min_len"],
    min_consec_allele = lambda wildcards: _params_sc2rf_recombinants(wildcards.build)["min_consec_allele"],
    max_breakpoint_len = lambda wildcards: _params_sc2rf_recombinants(wildcards.build)["max_breakpoint_len"],
    max_parents     = lambda wildcards: _params_sc2rf_recombinants(wildcards.build)["max_parents"],
    max_breakpoints = lambda wildcards: _params_sc2rf_recombinants(wildcards.build)["max_breakpoints"],
    motifs          = lambda wildcards: _params_sc2rf_recombinants(wildcards.build)["motifs"],
    dup_method      = lambda wildcards: _params_sc2rf_recombinants(wildcards.build)["dup_method"],
    auto_pass       = lambda wildcards: _params_sc2rf_recombinants(wildcards.build)["auto_pass"],
    lapis           = lambda wildcards: _params_sc2rf_recombinants(wildcards.build)["lapis"],
    metadata        = lambda wildcards: _params_sc2rf_recombinants(wildcards.build)["metadata"],
    # Join inputs together with commas
    ansi            = lambda wildcards: ",".join(_inputs_sc2rf_recombinants(wildcards.build)["ansi"]),
    csv             = lambda wildcards: ",".join(_inputs_sc2rf_recombinants(wildcards.build)["csv"]),
  threads: 1
  resources:
    cpus = 1,
  benchmark:
    "benchmarks/{rule}/{{build}}_{today}.tsv".format(today=today, rule=rule_name),
  log:
    "logs/{rule}/{{build}}_{today}.log".format(today=today, rule=rule_name),
  shell:
    """
    python3 sc2rf/postprocess.py \
      --csv {params.csv} \
      --ansi {params.ansi} \
      --prefix {params.prefix} \
      --outdir {params.outdir} \
      --aligned {input.alignment} \
      --issues {input.issues} \
      --nextclade {input.nextclade} \
      --nextclade-no-recomb {input.nextclade_no_recomb} \
      --lineage-tree {input.lineage_tree} \
      {params.metadata} \
      {params.auto_pass} \
      {params.motifs} \
      {params.min_len} \
      {params.min_consec_allele} \
      {params.max_breakpoint_len} \
      {params.max_parents} \
      {params.max_breakpoints} \
      {params.dup_method} \
      {params.lapis} \
      --log {log} \
      >> {log} 2>&1;

    # Cleanup
    mv -f {params.outdir}/recombinants.tsv {params.outdir}/stats.tsv >> {log} 2>&1;
    """

# ------------------------------------------------------------------------------
#  STAGE 4: Summarize and Report
# ------------------------------------------------------------------------------

rule_name = "summary"

# Parameters function
def _params_summary(build):
  """Parse parameters from wildcards for rule summary."""

  params = {}

  extra_cols = config["builds"][build]["summary"]["extra_cols"]
  if extra_cols: params["extra_cols"] = "--extra-cols {}".format(",".join(extra_cols))
  else: params["extra_cols"] = ""

  return params

# Snakemake rule
rule summary:
  """Summarize results from pipeline tools."""

  message: """Summarizing results from pipeline tools.\n
  build:      {wildcards.build}
  log:        {log}
  summary:    {output.summary}
  """

  input:
    nextclade        = "results/{build}/nextclade/metadata.tsv",
    sc2rf            = rules.sc2rf_recombinants.output.stats,
    rbd_levels       = rules.rbd_levels.output.table,
  output:
    summary = "results/{build}/linelists/summary.tsv",
  params:
    extra_cols         = lambda wildcards: _params_summary(wildcards.build)["extra_cols"],
    nextclade_dataset  = lambda wildcards: config["builds"][wildcards.build]["nextclade_dataset"]["dataset"],
    nextclade_tag      = lambda wildcards: config["builds"][wildcards.build]["nextclade_dataset"]["tag"],
  threads: 1
  resources:
    cpus = 1,
  benchmark:
    "benchmarks/{rule}/{{build}}_{today}.tsv".format(today=today, rule=rule_name),
  log:
    "logs/{rule}/{{build}}_{today}.log".format(today=today, rule=rule_name),
  shell:
    """
    bash scripts/summary.sh \
      --output {output.summary} \
      --nextclade {input.nextclade} \
      --sc2rf {input.sc2rf} \
      --rbd-levels {input.rbd_levels} \
      --nextclade-dataset {params.nextclade_dataset}_{params.nextclade_tag} \
      {params.extra_cols} \
      > {log} 2>&1;
    """

# ------------------------------------------------------------------------------
# Linelist

# Parameters Function
def _params_linelist(build):
  """Parse parameters from wildcards for rule linelist."""

  params = {}

  geo = config["builds"][build]["linelist"]["geo"]
  if geo: params["geo"] = "--geo {}".format(geo)
  else: params["geo"] = ""

  min_lineage_size = config["builds"][build]["linelist"]["min_lineage_size"]
  if min_lineage_size: params["min_lineage_size"] = "--min-lineage-size {}".format(min_lineage_size)
  else: params["min_lineage_size"] = ""

  min_private_muts = config["builds"][build]["linelist"]["min_private_muts"]
  if min_private_muts: params["min_private_muts"] = "--min-private-muts {}".format(min_private_muts)
  else: params["min_private_muts"] = ""

  return params


rule_name = "linelist"
rule linelist:
  """Create linelists of recombinant sequences, lineages, and parents."""

  message: """Creating linelists of recombinant sequences, lineages, and parents.\n
  build:             {wildcards.build}
  log:               {log}
  positives:         {output.positives}
  negatives:         {output.negatives}
  false_positives:   {output.false_positives}
  lineages:          {output.lineages}
  parents:           {output.parents}
  """

  input:
    summary         = rules.summary.output.summary,
    issues          = rules.issues_download.output.issues,
    lineage_tree    = rules.lineage_tree.output.tree,
  output:
    linelist        = "results/{build}/linelists/linelist.tsv",
    positives       = "results/{build}/linelists/positives.tsv",
    negatives       = "results/{build}/linelists/negatives.tsv",
    false_positives = "results/{build}/linelists/false_positives.tsv",
    lineages        = "results/{build}/linelists/lineages.tsv",
    parents         = "results/{build}/linelists/parents.tsv",
  params:
    outdir         = "results/{build}/linelists",
    extra_cols     = lambda wildcards: _params_summary(wildcards.build)["extra_cols"],
    geo            = lambda wildcards: _params_linelist(wildcards.build)["geo"],
    min_lineage_size = lambda wildcards: _params_linelist(wildcards.build)["min_lineage_size"],
    min_private_muts = lambda wildcards: _params_linelist(wildcards.build)["min_private_muts"],
  threads: 1
  resources:
    cpus = 1,
  benchmark:
    "benchmarks/{rule}/{{build}}_{today}.tsv".format(today=today, rule=rule_name),
  log:
    "logs/{rule}/{{build}}_{today}.log".format(today=today, rule=rule_name),
  shell:
    """
    # Create the strain tables
    python3 scripts/linelist.py \
      --input {input.summary} \
      --issues {input.issues} \
      --lineage-tree {input.lineage_tree} \
      --outdir {params.outdir} \
      {params.extra_cols} \
      {params.min_lineage_size} \
      {params.min_private_muts} \
      > {log} 2>&1;

    # Create the lineages table
    python3 scripts/lineages.py --input {output.positives} --output {output.lineages} {params.geo} >> {log} 2>&1;

    # Create the parents table
    python3 scripts/parents.py --input {output.positives} --output {output.parents} >> {log} 2>&1;
    """

# ------------------------------------------------------------------------------
# Plot

rule_name = "plot"

# Parameters Function
def _params_plot(build):
  """Parse parameters from wildcards for rule plot."""

  params = {}

  weeks = config["builds"][build]["plot"]["weeks"]
  if weeks: params["weeks"] = "--weeks {}".format(weeks)
  else: params["weeks"] = ""

  min_date = config["builds"][build]["plot"]["min_date"]
  if min_date: params["min_date"] = "--min-date {}".format(min_date)
  else: params["min_date"] = ""

  max_date = config["builds"][build]["plot"]["max_date"]
  if max_date: params["max_date"] = "--max-date {}".format(max_date)
  else: params["max_date"] = ""

  singletons = config["builds"][build]["plot"]["singletons"]
  if singletons: params["singletons"] = "--singletons"
  else: params["singletons"] = ""

  lag = config["builds"][build]["plot"]["lag"]
  if lag: params["lag"] = "--lag {}".format(lag)
  else: params["lag"] = ""

  return params

# Snakemake Rule
rule plot:
  """Plot results."""

  message: """Plotting results.\n
  build:   {wildcards.build}
  log:     {log}
  plots:   {output.plots}
  """

  input:
    positives  = rules.linelist.output.positives,
    lineages   = rules.linelist.output.lineages,
  output:
    plots      = directory("results/{build}/plots"),
  params:
    outdir     = "results/{build}/plots",
    geo        = lambda wildcards: _params_linelist(wildcards.build)["geo"],
    weeks      = lambda wildcards: _params_plot(wildcards.build)["weeks"],
    min_date   = lambda wildcards: _params_plot(wildcards.build)["min_date"],
    max_date   = lambda wildcards: _params_plot(wildcards.build)["max_date"],
    lag        = lambda wildcards: _params_plot(wildcards.build)["lag"],
    singletons = lambda wildcards: _params_plot(wildcards.build)["singletons"],
  threads: 1
  resources:
    cpus = 1,
  benchmark:
    "benchmarks/{rule}/{{build}}_{today}.tsv".format(today=today, rule=rule_name),
  log:
    "logs/{rule}/{{build}}_{today}.log".format(today=today, rule=rule_name),
  shell:
    """
    python3 scripts/plot.py \
      --input {input.positives} \
      --outdir {params.outdir} \
      {params.lag} \
      {params.geo} \
      {params.weeks} \
      {params.min_date} \
      {params.max_date} \
      {params.singletons} \
      > {log} 2>&1;

    # Extract the cluster IDs to be plotted
    cluster_ids=$(csvtk headers -t {output.plots}/cluster_id.tsv | tail -n+2 | tr "\\n" "," | sed 's/,$/\\n/g')

    # Plot breakpoints by clade
    python3 scripts/plot_breakpoints.py \
      --lineages {input.lineages} \
      --lineage-col recombinant_lineage_curated \
      --positives {input.positives} \
      --outdir {params.outdir} \
      --parent-col parents_clade \
      --parent-type clade \
      --cluster-col cluster_id \
      --clusters "${{cluster_ids}}" \
      >> {log} 2>&1;

    # Plot breakpoints by lineage
    python3 scripts/plot_breakpoints.py \
      --lineages {input.lineages} \
      --lineage-col recombinant_lineage_curated \
      --positives {input.positives} \
      --outdir {params.outdir} \
      --parent-col parents_lineage \
      --parent-type lineage \
      --cluster-col cluster_id \
      --clusters "${{cluster_ids}}" \
      >> {log} 2>&1;
    """

# ------------------------------------------------------------------------------
rule_name = "plot_historical"
rule plot_historical:
  """Plot results as a historical timeline."""

  message: """Plot results as a historical timeline.\n
  build:   {wildcards.build}
  log:     {log}
  plots:   {output.plots}
  """

  input:
    positives  = rules.linelist.output.positives,
    lineages   = rules.linelist.output.lineages,
  output:
    plots      = directory("results/{build}/plots_historical"),
  params:
    outdir     = "results/{build}/plots_historical",
    geo        = lambda wildcards: _params_linelist(wildcards.build)["geo"],
    lag        = lambda wildcards: _params_plot(wildcards.build)["lag"],
    singletons = lambda wildcards: _params_plot(wildcards.build)["singletons"],
  threads: 1
  resources:
    cpus = 1,
  benchmark:
    "benchmarks/{rule}/{{build}}_{today}.tsv".format(today=today, rule=rule_name),
  log:
    "logs/{rule}/{{build}}_{today}.log".format(today=today, rule=rule_name),
  shell:
    """
    python3 scripts/plot.py \
      --input {input.positives} \
      --outdir {params.outdir} \
      {params.lag} \
      {params.geo} \
      {params.singletons} \
      > {log} 2>&1;

    # Plot breakpoints by clade
    python3 scripts/plot_breakpoints.py \
      --lineages {input.lineages} \
      --lineage-col recombinant_lineage_curated \
      --positives {input.positives} \
      --outdir {params.outdir} \
      --parent-col parents_clade \
      --parent-type clade \
      --cluster-col cluster_id \
      --autoscale \
      >> {log} 2>&1;

    # Plot breakpoints by lineage
    python3 scripts/plot_breakpoints.py \
      --lineages {input.lineages} \
      --lineage-col recombinant_lineage_curated \
      --positives {input.positives} \
      --outdir {params.outdir} \
      --parent-col parents_lineage \
      --parent-type lineage \
      --cluster-col cluster_id \
      --autoscale \
      >> {log} 2>&1;
    """

# ------------------------------------------------------------------------------
# Report
# ------------------------------------------------------------------------------

rule_name = "report"

def _params_report(build):
  """Parse parameters from wildcards for rule report."""

  params = {}

  geo = config["builds"][build]["linelist"]["geo"]
  if geo: params["geo"] = "--geo {}".format(geo)
  else: params["geo"] = ""

  template = config["builds"][build]["report"]["template"]
  if template: params["template"] = "--template {}".format(template)
  else: params["template"] = ""

  return params

rule report:
  """Summarize results into a report."""

  message: """Creating excel report and powerpoint slides.\n
  build:   {wildcards.build}
  log:     {log}
  report:  {output.xlsx}
  slides:  {output.pptx}
  """

  input:
    plots           = rules.plot.output.plots,
    linelist        = rules.linelist.output.linelist,
    tables          = [
                      rules.linelist.output.lineages,
                      rules.linelist.output.parents,
                      rules.linelist.output.linelist,
                      rules.linelist.output.positives,
                      rules.linelist.output.negatives,
                      rules.linelist.output.false_positives,
                      rules.summary.output.summary,
                      rules.issues_download.output.issues,
                      ],
  output:
    pptx            = "results/{build}/report/report.pptx",
    xlsx            = "results/{build}/report/report.xlsx",
  params:
    geo       = lambda wildcards: _params_report(wildcards.build)["geo"],
    template  = lambda wildcards: _params_report(wildcards.build)["template"],
    singletons = lambda wildcards: _params_plot(wildcards.build)["singletons"],

  threads: 1
  resources:
    cpus = 1,
  benchmark:
    "benchmarks/{rule}/{{build}}_{today}.tsv".format(today=today, rule=rule_name),
  log:
    "logs/{rule}/{{build}}_{today}.log".format(today=today, rule=rule_name),
  shell:
    """
    # Create the excel report
    csvtk csv2xlsx -t -o {output.xlsx} {input.tables} > {log} 2>&1;

    # Create the powerpoint slides
    python3 scripts/report.py --linelist {input.linelist} --plot-dir {input.plots} --output {output.pptx} {params.geo} {params.template} {params.singletons} >> {log} 2>&1;
    """

rule report_historical:
  """Summarize results into a historical report."""

  message: """Summarize results into a historical report.\n
  build:   {wildcards.build}
  log:     {log}
  slides:  {output.pptx}
  """

  input:
    plots           = rules.plot_historical.output.plots,
    linelist        = rules.linelist.output.linelist,
  output:
    pptx            = "results/{build}/report_historical/report.pptx",
  params:
    geo       = lambda wildcards: _params_report(wildcards.build)["geo"],
    template  = lambda wildcards: _params_report(wildcards.build)["template"],
    singletons = lambda wildcards: _params_plot(wildcards.build)["singletons"],

  threads: 1
  resources:
    cpus = 1,
  benchmark:
    "benchmarks/{rule}/{{build}}_{today}.tsv".format(today=today, rule=rule_name),
  log:
    "logs/{rule}/{{build}}_{today}.log".format(today=today, rule=rule_name),
  shell:
    """
    # Create the powerpoint slides
    python3 scripts/report.py --linelist {input.linelist} --plot-dir {input.plots} --output {output.pptx} {params.geo} {params.template} {params.singletons} > {log} 2>&1;
    """

# ------------------------------------------------------------------------------
# Validation
# ------------------------------------------------------------------------------

rule_name = "validate"

def _inputs_validate(build):
  inputs = {}

  expected = config["builds"][build]["validate"]["expected"]
  inputs["expected"] = expected

  return inputs

rule validate:
  """Validate observed values in linelist."""

  message: """Validating observed values in linelist.\n
  build:    {wildcards.build}
  log:      {log}
  expected: {input.expected}
  table:    {output.table}
  """

  input:
    expected = lambda wildcards: _inputs_validate(wildcards.build)["expected"],
    observed = "results/{build}/linelists/linelist.tsv",
  output:
   table     = "results/{build}/validate/validation.tsv",
   status    = "results/{build}/validate/status.txt",
  params:
    outdir   = "results/{build}/validate",
  threads: 1
  resources:
    cpus = 1,
  benchmark:
    "benchmarks/{rule}/{{build}}_{today}.tsv".format(today=today, rule=rule_name),
  log:
    "logs/{rule}/{{build}}_{today}.log".format(today=today, rule=rule_name),
  shell:
    """
    python scripts/validate.py --expected {input.expected} --observed {input.observed} --outdir {params.outdir};

    # Throw a pipeline error if any sample has "Fail"
    build_status=$(cat {output.status})

    if [[ $build_status == "Fail" ]]; then

      echo "Build failed validation: {wildcards.build}" > {log}
      csvtk grep -t -f "status" -p "Fail" {output.table} >> {log}
      exit 1
    fi
    """
